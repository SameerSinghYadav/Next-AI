{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPXFKCI47vq5F0PJxUxwrCg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SameerSinghYadav/Next-AI/blob/main/NLP_based_CHATBOT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "E9OPOXIDcER5"
      },
      "outputs": [],
      "source": [
        "#tokenization\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "#stopwords list\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import PorterStemmer\n",
        "#stemming\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "import string\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.linear_model import LogisticRegression"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "training_data = [(\"hello\",\"greet\"),\n",
        "                 (\"hello there\",\"greet\"),\n",
        "                 (\"hey\",\"greet\"),\n",
        "                 (\"good morning\",\"greet\"),\n",
        "                 (\"what's the weather today\",\"weather\"),\n",
        "                 (\"what's the temperature\",\"weather\"),\n",
        "                 (\"is it raining\",\"weather\"),\n",
        "                 (\"open google\",\"open_web\"),\n",
        "                 (\"open facebook\",\"open_web\"),\n",
        "                 (\"open youtube\",\"open_web\"),\n",
        "                 (\"bye\",\"exit\"),\n",
        "                 (\"googdbye\",\"exit\"),\n",
        "                 (\"exit\",\"exit\")\n",
        "                 ]"
      ],
      "metadata": {
        "id": "B2GFTABef6cv"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen=[] #features/x\n",
        "lab=[] #target/y\n",
        "\n",
        "for text, intent in training_data:\n",
        "  sen.append(text)\n",
        "  lab.append(intent)"
      ],
      "metadata": {
        "id": "g_xfXz0igXPP"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_text(documents):\n",
        "    Cleaned_data = []\n",
        "\n",
        "    english_stopwords = stopwords.words(\"english\")\n",
        "    wnet = WordNetLemmatizer()\n",
        "\n",
        "    for doc in documents:\n",
        "        filtered_tokens = []\n",
        "        final_tokens = []\n",
        "\n",
        "        raw_text = doc.lower()\n",
        "        print(\"After lower case : \", raw_text)\n",
        "\n",
        "        tokens = word_tokenize(raw_text)\n",
        "        print(\"Tokens:\", tokens)\n",
        "\n",
        "        for word in tokens:\n",
        "            if word not in english_stopwords:\n",
        "                filtered_tokens.append(word)\n",
        "\n",
        "        print(\"Filtered Tokens :\", filtered_tokens)\n",
        "\n",
        "        punctuations = string.punctuation\n",
        "        clean_tokens = [word for word in filtered_tokens if word not in punctuations]\n",
        "\n",
        "        lemmatized_words = []\n",
        "        for word in clean_tokens:\n",
        "            lemmatized_words.append(wnet.lemmatize(word, \"v\"))\n",
        "\n",
        "        for word in lemmatized_words:\n",
        "            if word.isalpha():\n",
        "                final_tokens.append(word)\n",
        "\n",
        "        cleaned_text = \" \".join(final_tokens)\n",
        "\n",
        "        Cleaned_data.append(cleaned_text)\n",
        "\n",
        "    return Cleaned_data"
      ],
      "metadata": {
        "id": "SBWOcQ-fh8R5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sen[:5]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NJANVYlUjUTn",
        "outputId": "393e2c9b-41c0-46ff-d94e-5bb76d3681ca"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['hello', 'hello there', 'hey', 'good morning', \"what's the weather today\"]"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('punkt_tab')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uhgXlOLtkVfc",
        "outputId": "778f70c4-d876-4e30-f901-1a34da5648ec"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cleared_data = preprocess_text(sen)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0BUU0GqkZsy",
        "outputId": "34261810-5df5-43d0-b90b-fc734e2fbecd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After lower case :  hello\n",
            "Tokens: ['hello']\n",
            "Filtered Tokens : ['hello']\n",
            "After lower case :  hello there\n",
            "Tokens: ['hello', 'there']\n",
            "Filtered Tokens : ['hello']\n",
            "After lower case :  hey\n",
            "Tokens: ['hey']\n",
            "Filtered Tokens : ['hey']\n",
            "After lower case :  good morning\n",
            "Tokens: ['good', 'morning']\n",
            "Filtered Tokens : ['good', 'morning']\n",
            "After lower case :  what's the weather today\n",
            "Tokens: ['what', \"'s\", 'the', 'weather', 'today']\n",
            "Filtered Tokens : [\"'s\", 'weather', 'today']\n",
            "After lower case :  what's the temperature\n",
            "Tokens: ['what', \"'s\", 'the', 'temperature']\n",
            "Filtered Tokens : [\"'s\", 'temperature']\n",
            "After lower case :  is it raining\n",
            "Tokens: ['is', 'it', 'raining']\n",
            "Filtered Tokens : ['raining']\n",
            "After lower case :  open google\n",
            "Tokens: ['open', 'google']\n",
            "Filtered Tokens : ['open', 'google']\n",
            "After lower case :  open facebook\n",
            "Tokens: ['open', 'facebook']\n",
            "Filtered Tokens : ['open', 'facebook']\n",
            "After lower case :  open youtube\n",
            "Tokens: ['open', 'youtube']\n",
            "Filtered Tokens : ['open', 'youtube']\n",
            "After lower case :  bye\n",
            "Tokens: ['bye']\n",
            "Filtered Tokens : ['bye']\n",
            "After lower case :  googdbye\n",
            "Tokens: ['googdbye']\n",
            "Filtered Tokens : ['googdbye']\n",
            "After lower case :  exit\n",
            "Tokens: ['exit']\n",
            "Filtered Tokens : ['exit']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "h_yMsbOwk4te"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}